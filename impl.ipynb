{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy\n",
    "import sympy as sp\n",
    "from IPython.display import display\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deriving discrete dynamics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_continuous = sp.Matrix([[0, 1], [0, 0]])\n",
    "B_continuous = sp.Matrix([[0], [1]])\n",
    "dt = sp.symbols('dt')\n",
    "x1, x2 = sp.symbols('x1 x2')\n",
    "x = sp.Matrix([[x1], [x2]])\n",
    "u = sp.symbols('u')\n",
    "\n",
    "k1 = A_continuous * x + B_continuous * u\n",
    "k2 = A_continuous * (x + 0.5 * dt * k1) + B_continuous * u\n",
    "k3 = A_continuous * (x + 0.5 * dt * k2) + B_continuous * u\n",
    "k4 = A_continuous * (x + dt * k3) + B_continuous * u\n",
    "\n",
    "x_new = sp.simplify(x + dt / 6 * (k1 + 2 * k2 + 2 * k3 + k4))\n",
    "display(x_new)\n",
    "\n",
    "A_discrete = sp.Matrix([[1, dt], [0, 1]])\n",
    "B_discrete = sp.Matrix([[dt**2 / 2], [dt]])\n",
    "print(\"A_discrete\")\n",
    "display(A_discrete)\n",
    "print(\"B_discrete\")\n",
    "display(B_discrete)\n",
    "display(A_discrete * x + B_discrete * u)\n",
    "display(A_discrete * x + B_discrete * u - x_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute Sigma_K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def x0_distribution():\n",
    "    return np.random.multivariate_normal(np.zeros(2), np.eye(2))\n",
    "\n",
    "def get_sigma_k_exact(A,B,K):\n",
    "    closed_loop = A - B @ K\n",
    "    return scipy.linalg.solve_discrete_lyapunov(closed_loop, np.eye(2))\n",
    "\n",
    "def get_sigma_k_mc(A,B,K,x0_distribution,num_samples=1000,max_timesteps=50):\n",
    "    total = np.zeros((2,2))\n",
    "    for i in range(num_samples):\n",
    "        x0 = x0_distribution()\n",
    "        tt = A - B @ K\n",
    "        for j in range(max_timesteps):\n",
    "            total += np.outer(x0, x0)\n",
    "            x0 = tt @ x0\n",
    "    return total / num_samples\n",
    "\n",
    "dt = 0.1\n",
    "A = np.array([[1, dt], [0, 1]])\n",
    "B = np.array([[dt**2 / 2], [dt]])\n",
    "K = np.array([[0.5, 0.3]])\n",
    "\n",
    "print(\"Exact\")\n",
    "print(get_sigma_k_exact(A,B,K))\n",
    "\n",
    "sanity_check = True\n",
    "if sanity_check:    \n",
    "    print(\"MC\")\n",
    "    print(get_sigma_k_mc(A,B,K,x0_distribution, num_samples=3000, max_timesteps=200))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute P_K and CK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_P_K(K, A, B, Q, R):\n",
    "    return scipy.linalg.solve_discrete_lyapunov((A - B @ K).T, Q + K.T @ R @ K)\n",
    "\n",
    "def get_cost(K, A, B, Q, R):\n",
    "    return np.trace(get_P_K(K, A, B, Q, R))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discrete Double Integrator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = 0.1\n",
    "A = np.array([[1, dt], [0, 1]])\n",
    "B = np.array([[dt**2 / 2], [dt]])\n",
    "Q = np.eye(2) * 2\n",
    "R = np.array([[1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check gradient domination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_gradient_domination(K, K_opt, A, B, Q, R, grad_C_K):    \n",
    "    Ck = get_cost(K, A, B, Q, R)\n",
    "    Ck_opt = get_cost(K_opt, A, B, Q, R)\n",
    "    Sigma_K = get_sigma_k_exact(A,B,K)\n",
    "    Sigma_K_opt = get_sigma_k_exact(A,B,K_opt)\n",
    "\n",
    "    # Get minimum singular value of Sigma_K_opt\n",
    "    min_sv_K = np.min(np.linalg.svd(Sigma_K)[1])\n",
    "    min_sv_R = np.min(np.linalg.svd(R)[1])\n",
    "\n",
    "    C_diff = Ck - Ck_opt\n",
    "    ub = np.linalg.norm(Sigma_K_opt, ord=2)\n",
    "    ub = ub / (min_sv_K**2 * min_sv_R) * np.linalg.norm(grad_C_K)**2\n",
    "    assert ub > C_diff, f\"C_diff_upper: {ub}, C_diff: {C_diff}\"\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exact Policy Gradient Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_optimal_K(A,B,Q,R,alpha, K0 = np.array([[1, 1]]), max_iterations=1000):\n",
    "    \n",
    "    K = K0\n",
    "    \n",
    "    for i in range(max_iterations):\n",
    "        P_K = get_P_K(K, A, B, Q, R)\n",
    "\n",
    "        # Compute Sigma_K\n",
    "        Sigma_K = get_sigma_k_exact(A,B,K)\n",
    "        \n",
    "        # Compute E_K\n",
    "        E_K = (R + B.T @ P_K @ B) @ K - B.T @ P_K @ A\n",
    "        \n",
    "        # Compute gradient\n",
    "        grad_C_K = 2 * E_K @ Sigma_K\n",
    "        \n",
    "        # Gauss-Newton update\n",
    "        K_new = K - alpha * scipy.linalg.inv(R + B.T @ P_K @ B) @ grad_C_K @ scipy.linalg.inv(Sigma_K)\n",
    "        \n",
    "        K = K_new\n",
    "    \n",
    "    return K\n",
    "\n",
    "K_opt = compute_optimal_K(A,B,Q,R,alpha=0.02, max_iterations=5000)\n",
    "print(f\"Optimal K: {K_opt}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exact_policy_gradient(A,B,Q,R,alpha, K0 = np.array([[1,1]]), max_iterations=1000):\n",
    "    \n",
    "    K = K0\n",
    "    distances = []\n",
    "    \n",
    "    for i in range(max_iterations):\n",
    "        P_K = get_P_K(K, A, B, Q, R)\n",
    "\n",
    "        # Compute Sigma_K\n",
    "        Sigma_K = get_sigma_k_exact(A,B,K)\n",
    "        \n",
    "        # Compute E_K\n",
    "        E_K = (R + B.T @ P_K @ B) @ K - B.T @ P_K @ A\n",
    "        \n",
    "        # Compute gradient\n",
    "        grad_C_K = 2 * E_K @ Sigma_K\n",
    "\n",
    "        if np.linalg.norm(grad_C_K) > 1e-4:\n",
    "            check_gradient_domination(K, K_opt, A, B, Q, R, grad_C_K)\n",
    "        \n",
    "        # Gauss-Newton update\n",
    "        K_new = K - alpha * grad_C_K\n",
    "        \n",
    "        # Store distance to optimal K\n",
    "        distances.append(np.linalg.norm(K - K_opt))\n",
    "        \n",
    "        K = K_new\n",
    "    \n",
    "    return K, distances\n",
    "\n",
    "K, distances_pg = exact_policy_gradient(A,B,Q,R,alpha=0.02, max_iterations=3000)\n",
    "\n",
    "# Plot distances over iterations\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(distances_pg)\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Distance to Optimal K')\n",
    "plt.title('Convergence of Policy Gradient')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Optimal K: {K_opt}\")\n",
    "print(f\"Computed K: {K}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exact_natural_policy_gradient(A,B,Q,R,alpha, K0 = np.array([[1, 1]]), max_iterations=1000):\n",
    "    \n",
    "    K = K0\n",
    "    distances = []\n",
    "    \n",
    "    for i in range(max_iterations):\n",
    "        # Compute P_K\n",
    "        P_K = get_P_K(K, A, B, Q, R)\n",
    "        \n",
    "        # Compute Sigma_K\n",
    "        Sigma_K = get_sigma_k_exact(A,B,K)\n",
    "        \n",
    "        # Compute E_K\n",
    "        E_K = (R + B.T @ P_K @ B) @ K - B.T @ P_K @ A\n",
    "        \n",
    "        # Compute gradient\n",
    "        grad_C_K = 2 * E_K @ Sigma_K\n",
    "\n",
    "        if np.linalg.norm(grad_C_K) > 1e-4:\n",
    "            check_gradient_domination(K, K_opt, A, B, Q, R, grad_C_K)\n",
    "        \n",
    "        # Gauss-Newton update\n",
    "        K_new = K - alpha * grad_C_K @ scipy.linalg.inv(Sigma_K)\n",
    "\n",
    "        # Store distance to optimal K\n",
    "        distances.append(np.linalg.norm(K - K_opt))\n",
    "        \n",
    "        K = K_new\n",
    "    \n",
    "    return K, distances\n",
    "\n",
    "K, distances_ng = exact_natural_policy_gradient(A,B,Q,R,alpha=0.02, max_iterations=3000)\n",
    "\n",
    "# Plot distances over iterations\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(distances_ng)\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Distance to Optimal K')\n",
    "plt.title('Convergence of Natural Policy Gradient')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Optimal K: {K_opt}\")\n",
    "print(f\"Computed K: {K}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exact_gauss_newton(A,B,Q,R,alpha, K0 = np.array([[1, 1]]), max_iterations=1000):\n",
    "    \n",
    "    K = K0\n",
    "    distances = []\n",
    "    \n",
    "    for i in range(max_iterations):\n",
    "        # Compute P_K\n",
    "        P_K = get_P_K(K, A, B, Q, R)\n",
    "        \n",
    "        # Compute Sigma_K\n",
    "        Sigma_K = get_sigma_k_exact(A,B,K)\n",
    "        \n",
    "        # Compute E_K\n",
    "        E_K = (R + B.T @ P_K @ B) @ K - B.T @ P_K @ A\n",
    "        \n",
    "        # Compute gradient\n",
    "        grad_C_K = 2 * E_K @ Sigma_K\n",
    "\n",
    "        if np.linalg.norm(grad_C_K) > 1e-4:\n",
    "            check_gradient_domination(K, K_opt, A, B, Q, R, grad_C_K)\n",
    "        \n",
    "        # Gauss-Newton update\n",
    "        K_new = K - alpha * scipy.linalg.inv(R + B.T @ P_K @ B) @ grad_C_K @ scipy.linalg.inv(Sigma_K)\n",
    "        \n",
    "        # Store distance to optimal K\n",
    "        distances.append(np.linalg.norm(K - K_opt))\n",
    "        \n",
    "        K = K_new\n",
    "    \n",
    "    return K, distances\n",
    "\n",
    "K, distances_gn = exact_gauss_newton(A,B,Q,R,alpha=0.02, max_iterations=3000)\n",
    "\n",
    "# Plot distances over iterations\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(distances_gn)\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Distance to Optimal K')\n",
    "plt.title('Convergence of Gauss-Newton Method')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Optimal K: {K_opt}\")\n",
    "print(f\"Computed K: {K}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 300\n",
    "font_size = 17\n",
    "line_width = 3\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(distances_pg[:N], label=\"Policy Gradient\", linewidth=line_width)\n",
    "plt.plot(distances_ng[:N], label=\"Natural Policy Gradient\", linewidth=line_width) \n",
    "plt.plot(distances_gn[:N], label=\"Gauss-Newton\", linewidth=line_width)\n",
    "plt.xlabel('Iteration', fontsize=font_size)\n",
    "plt.ylabel('Distance to Optimal K', fontsize=font_size)\n",
    "plt.title('Convergence of Different Methods', fontsize=font_size)\n",
    "plt.grid(True)\n",
    "plt.grid(True, linestyle='--', alpha=0.7, which='both')\n",
    "plt.minorticks_on()\n",
    "plt.grid(True, which='minor', linestyle=':', alpha=0.4)\n",
    "plt.legend(fontsize=font_size)\n",
    "plt.savefig(\"convergence.png\", dpi=300)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load distances from file\n",
    "with open(\"distances_pg.txt\", \"r\") as file:\n",
    "    distances_pg = [float(line.strip()) for line in file]\n",
    "\n",
    "with open(\"distances_npg.txt\", \"r\") as file:\n",
    "    distances_npg = [float(line.strip()) for line in file]\n",
    "# Compare distances\n",
    "N = 3000\n",
    "font_size = 17\n",
    "line_width = 3\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(distances_pg[:N], label=\"Approximate Policy Gradient\", linewidth=line_width)\n",
    "plt.plot(distances_npg[:N], label=\"Approximate Natural Policy Gradient\", linewidth=line_width)\n",
    "plt.xlabel('Iteration', fontsize=font_size)\n",
    "plt.ylabel('Distance to Optimal K', fontsize=font_size)\n",
    "plt.title('Convergence of Different Methods', fontsize=font_size)\n",
    "plt.grid(True)\n",
    "plt.grid(True, linestyle='--', alpha=0.7, which='both')\n",
    "plt.minorticks_on()\n",
    "plt.grid(True, which='minor', linestyle=':', alpha=0.4)\n",
    "plt.legend(fontsize=font_size)\n",
    "plt.savefig(\"convergence.png\", dpi=300)\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Approximate Policy Gradient Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Function to sample initial state from a standard normal distribution\n",
    "def sample_initial_state(d):\n",
    "    return np.random.normal(0, 1, size=(d,))\n",
    "\n",
    "# Function to compute Frobenius norm of a matrix\n",
    "def frobenius_norm(mat):\n",
    "    return np.sqrt(np.sum(np.square(mat)))\n",
    "\n",
    "def policy_gradient_estimation_naive(K, m, roll_out_length, r, d, A, B, Q, R):\n",
    "    C_estimates = []\n",
    "    sigma_estimates = []\n",
    "\n",
    "    for _ in range(m):\n",
    "        # Sample random matrix U_i with Frobenius norm r\n",
    "        U_i = np.random.uniform(-1, 1, size=K.shape)\n",
    "        U_i *= r / frobenius_norm(U_i)\n",
    "\n",
    "        # Perturbed policy K_i = K + U_i\n",
    "        K_i = K + U_i\n",
    "\n",
    "        # Initialize arrays for trajectory\n",
    "        total_cost = 0.0\n",
    "        states = []\n",
    "\n",
    "        # Sample initial state\n",
    "        x = sample_initial_state(d)\n",
    "\n",
    "        for _ in range(roll_out_length):\n",
    "            states.append(x)\n",
    "            u = -np.dot(K_i, x)\n",
    "            total_cost += np.dot(x.T, np.dot(Q, x)) + np.dot(u.T, np.dot(R, u))\n",
    "            x = np.dot(A, x) + np.dot(B, u)\n",
    "\n",
    "        # Calculate empirical estimates\n",
    "        C_i = total_cost\n",
    "        Sigma_i = np.zeros((d, d))\n",
    "        for s in states:\n",
    "            Sigma_i += np.outer(s, s)\n",
    "\n",
    "        C_estimates.append((C_i, U_i))\n",
    "        sigma_estimates.append(Sigma_i)\n",
    "\n",
    "    # Calculate final estimates\n",
    "    gradient_estimate = np.zeros(K.shape)\n",
    "    for C_i, U_i in C_estimates:\n",
    "        gradient_estimate += (d / (m * r * r)) * C_i * U_i\n",
    "\n",
    "    sigma_estimate = np.mean(sigma_estimates, axis=0)\n",
    "\n",
    "    return gradient_estimate, sigma_estimate\n",
    "\n",
    "def model_free_natural_policy_gradient(A, B, Q, R, K0, m, roll_out_length, r, max_iterations=1000, eta=0.01):\n",
    "    K = K0\n",
    "    d = A.shape[0]\n",
    "    \n",
    "    for _ in range(max_iterations):\n",
    "        # Get empirical estimates\n",
    "        grad_estimate, sigma_estimate = policy_gradient_estimation_naive(K, m, roll_out_length, r, d, A, B, Q, R)\n",
    "        \n",
    "        print(\"Gradient estimate:\\n\", grad_estimate)\n",
    "        \n",
    "        # Natural policy gradient update\n",
    "        K_new = K - eta * grad_estimate\n",
    "        \n",
    "        # Check convergence\n",
    "        if np.linalg.norm(K - K_new) < 1e-8:\n",
    "            break\n",
    "        \n",
    "        K = K_new\n",
    "        print(\"Current K:\\n\", K)\n",
    "    \n",
    "    return K\n",
    "\n",
    "d = 2\n",
    "m = 3000\n",
    "roll_out_length = 150\n",
    "r = 0.001\n",
    "\n",
    "dt = 0.1\n",
    "K = np.array([[1, 1]])\n",
    "A = np.array([[1, dt], [0, 1]])\n",
    "B = np.array([[0.5 * dt ** 2], [dt]])\n",
    "Q = np.eye(d) * 2\n",
    "R = np.eye(1)\n",
    "\n",
    "K0 = np.array([[1.0, 1.0]])\n",
    "\n",
    "print(\"Starting model-free natural policy gradient optimization...\")\n",
    "\n",
    "Kest = model_free_natural_policy_gradient(\n",
    "    A, B, Q, R, K0, \n",
    "    m=3000,\n",
    "    roll_out_length=200,\n",
    "    r=0.1,\n",
    "    max_iterations=10000,\n",
    "    eta=0.0005\n",
    ")\n",
    "\n",
    "print(\"Final K:\\n\", Kest)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
